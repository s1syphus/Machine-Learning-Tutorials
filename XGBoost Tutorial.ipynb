{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Library Tutorial\n",
    "Add in installation instructions (need to build and install)\n",
    "\n",
    "This is the multiclass classification demo from xgboost\n",
    "Can use same data but should be changed to be a little more instructive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.011719\ttest-merror:0.127273\n",
      "[1]\ttrain-merror:0.015625\ttest-merror:0.127273\n",
      "[2]\ttrain-merror:0.011719\ttest-merror:0.109091\n",
      "[3]\ttrain-merror:0.007812\ttest-merror:0.081818\n",
      "[4]\ttrain-merror:0.007812\ttest-merror:0.090909\n",
      "predicting, classification error=0.090909\n",
      "[0]\ttrain-merror:0.011719\ttest-merror:0.127273\n",
      "[1]\ttrain-merror:0.015625\ttest-merror:0.127273\n",
      "[2]\ttrain-merror:0.011719\ttest-merror:0.109091\n",
      "[3]\ttrain-merror:0.007812\ttest-merror:0.081818\n",
      "[4]\ttrain-merror:0.007812\ttest-merror:0.090909\n",
      "predicting, classification error=0.090909\n"
     ]
    }
   ],
   "source": [
    "# label need to be 0 to num_class -1\n",
    "data = np.loadtxt('Data/dermatology.data', delimiter=',',converters={33: lambda x:int(x == '?'), 34: lambda x:int(x)-1 } )\n",
    "sz = data.shape\n",
    "\n",
    "train = data[:int(sz[0] * 0.7), :]\n",
    "test = data[int(sz[0] * 0.7):, :]\n",
    "\n",
    "train_X = train[:,0:33]\n",
    "train_Y = train[:, 34]\n",
    "\n",
    "\n",
    "test_X = test[:,0:33]\n",
    "test_Y = test[:, 34]\n",
    "\n",
    "xg_train = xgb.DMatrix( train_X, label=train_Y)\n",
    "xg_test = xgb.DMatrix(test_X, label=test_Y)\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 6\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 5\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "# get prediction\n",
    "pred = bst.predict( xg_test );\n",
    "\n",
    "print ('predicting, classification error=%f' % (sum( int(pred[i]) != test_Y[i] for i in range(len(test_Y))) / float(len(test_Y)) ))\n",
    "\n",
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "yprob = bst.predict( xg_test ).reshape( test_Y.shape[0], 6 )\n",
    "ylabel = np.argmax(yprob, axis=1)\n",
    "\n",
    "print ('predicting, classification error=%f' % (sum( int(ylabel[i]) != test_Y[i] for i in range(len(test_Y))) / float(len(test_Y)) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
